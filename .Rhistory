dat <- gps_data %>%
filter(burst==burst_id)
# Print current burst
cat("****************************\n")
cat(paste0("Processing ", burst_id, "\n"))
cat("****************************\n\n")
# Order by date and assign location id
dat <- dat %>%
arrange(date) %>%
mutate(loc_id = 1:nrow(.)) %>%
dplyr::select(loc_id, everything())
# Handle dates
dates_out <- date_handler(dat, sea_start, sea_end)
# dates_out is a list; pull the data from it
dat <- dates_out$dat
# Handle cases where there are no data within the nesting season
if (nrow(dat) == 0) {
cat(paste0("Burst ", burst_id,
": no data available within the nesting season\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Calculate distance matrix
cat("Calculating distance matrix... ")
dmat <- dist_mat(dat)
cat("Done.\n")
# Get potential nest candidates
cat("Finding candidate nest locations for each buffer size... ")
cands_list <- get_candidates_multi(dmat = dmat, buffers = buffers, min_pts = min_pts)
cat("Done.\n")
# Remove distance matrix to free up RAM
rm(dmat)
gc()
# Create list to save results for each buffer
res_buffer <- list()
for (k in 1:length(cands_list)) {
# Need to start with a fresh 'dat' at every k
dat_buff <- dat
# Handle cases where there are no candidates
if (nrow(cands_list[[k]]) == 0) {
cat(paste0("Burst ", burst_id,
": no revisited locations found\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Summarize candidates
cands_count <- candidate_summary(cands = cands_list[[k]])
# Join group ID back to the original data
dat_buff <- left_join(dat, cands_list[[k]], by = "loc_id")
# Save computation time: discard group IDs that appear on < 2 days
keepers <- dat_buff %>%
group_by(group_id, reldate) %>%
tally() %>%
filter(n >= 2) %>%
pull(group_id) %>%
unique()
# Subset data for group_ids of interest
sub <- dat_buff %>%
filter(group_id %in% keepers)
# Handle cases where there are no keepers
if (nrow(sub) == 0) {
cat(paste0("Burst ", burst_id,
": no locations revisited for more than ",
min_consec,
" days\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Calculate revisitation stats
cat(paste0("Calculating revisitation patterns for buffer ", k, " of ", length(buffers), "... "))
daily_stats <- revisit_stats(dat = dat_buff,
sub = sub,
sea_start = sea_start,
sea_end = sea_end,
min_d_fix = min_d_fix,
min_consec = min_consec,
nest_cycle = nest_cycle)
cat("Done.\n")
# Filter group_ids that satisfy input criteria and add coordinates
nests <- daily_stats %>%
filter(!is.na(attempt_start),
!is.na(attempt_end),
consec_days >= min_consec,
perc_days_vis >= min_days_att,
perc_top_vis >= min_top_att) %>%
left_join(dplyr::select(dat_buff, loc_id, long, lat), by = c("group_id" = "loc_id")) %>%
mutate(attempt_start = ymd(dates_out$actual_start) + attempt_start) %>%
mutate(attempt_end = ymd(dates_out$actual_start) + attempt_end) %>%
mutate(burst = burst_id) %>%
dplyr::select(burst,
loc_id = group_id,
long,
lat,
first_date,
last_date,
attempt_start,
attempt_end,
tot_vis,
days_vis,
consec_days,
perc_days_vis,
perc_top_vis) %>%
arrange(desc(tot_vis))
# Handle cases where no nests passed the filter
if (nrow(sub) == 0) {
cat(paste0("Burst ", burst_id,
": no locations found for the specified set of parameters\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Optional: deal with temporally overlapping attempts
if (discard_overlapping) {
nests <- choose_overlapping(nests)
}
# Store results for the current buffer
res_buffer[[k]] <- nests
}
names(res_buffer) <- buffers
cat("\nProcess completed!\n\n")
saveRDS(res_buffer, paste0(temp_name, "/buffer_comparison_", burst_id, ".rds"))
rm(res_buffer)
gc()
}
cat("\nCompleted processing of all bursts.\n")
# At the end
# Combine results
# Each file (per burst) is a list with an element per buffer.
# Convert into a list with an element per buffer where each element is a data.frame for all bursts.
files_buffers <- list.files(temp_name, pattern = "buffer_comparison_", full.names = TRUE)
buffercomp <- list(NA, NA)
for (f in files_buffers) {
temp <- readRDS(f)
buffercomp <- purrr::map2(buffercomp, temp, rbind)
}
buffercomp <- lapply(buffercomp, FUN = function(x){x <- x[-1,]; rownames(x) <- 1:nrow(x); return(x)})
names(buffercomp) <- buffers
# Delete temporary folder and files within it
file.remove(files_buffers)
file.remove(temp_name)
# Count results and compute performance metrics
#Total number of nests
tot_nests <- data.frame()
for (n in 1:(length(buffercomp))) {
temp <- cbind.data.frame(buffer_size = names(buffercomp[n]), n = nrow(buffercomp[[n]]))
tot_nests <- rbind(tot_nests, temp)
}
#Number of nests per individual
nests_per_ind <- data.frame()
for (n in 1:(length(buffercomp))) {
temp <- buffercomp[[n]] %>%
group_by(burst) %>%
tally() %>%
as.data.frame() %>%
cbind(buffer_size = names(buffercomp[n])) %>%
dplyr::select(burst, buffer_size, n)
nests_per_ind <- rbind(nests_per_ind, temp) %>%
arrange(burst)
}
# Fix column names
names(known_nest_coords) <- c("burst", "true_long", "true_lat")
str(known_nest_coords)
#Get rid of bursts that don't have at least 'min_consec' days of data
enough <- gps_data %>%
group_by(burst) %>%
summarize(days_data = length(unique(date(date)))) %>%
filter(days_data >= min_consec) %>%
pull(burst)
known_nest_coords <- known_nest_coords %>%
filter(burst %in% enough)
buffercomp_copy <- lapply(buffercomp, FUN = function(x){
y <- x[x$burst %in% enough,]
return(y)
})
#Positive predictive value
# Number of known nests found
ppv_num <- unlist(lapply(buffercomp_copy, FUN = function(x) {
y <- x %>%
left_join(known_nest_coords, by = "burst") %>%
mutate(dist = geosphere::distGeo(cbind(true_long, true_lat), cbind(long, lat))) %>%
filter(dist <= 100) %>%
nrow()
return(y)
}))
str(buffercomp_copy[[1]])
# Change burst to character if it is not already
known_nest_coords$burst %>% class()
# Change burst to character if it is not already
known_nest_coords$burst <- as.character(known_nest_coords$burst)
# Change burst to character if it is not already
known_nest_coords$burst %>% class()
#Get rid of bursts that don't have at least 'min_consec' days of data
enough <- gps_data %>%
group_by(burst) %>%
summarize(days_data = length(unique(date(date)))) %>%
filter(days_data >= min_consec) %>%
pull(burst)
known_nest_coords <- known_nest_coords %>%
filter(burst %in% enough)
buffercomp_copy <- lapply(buffercomp, FUN = function(x){
y <- x[x$burst %in% enough,]
return(y)
})
#Positive predictive value
# Number of known nests found
ppv_num <- unlist(lapply(buffercomp_copy, FUN = function(x) {
y <- x %>%
left_join(known_nest_coords, by = "burst") %>%
mutate(dist = geosphere::distGeo(cbind(true_long, true_lat), cbind(long, lat))) %>%
filter(dist <= 100) %>%
nrow()
return(y)
}))
# Total number of nests found
ppv_den <- unlist(lapply(buffercomp_copy, FUN = function(x) {
y <- x %>%
filter(burst %in% known_nest_coords$burst) %>%
nrow()}))
# PPV
ppv <- data.frame(ppv = ppv_num/ppv_den*100)
ppv <- cbind(ppv, buffer_size = rownames(ppv)) %>%
dplyr::select(buffer_size, ppv)
rownames(ppv) <- NULL
compare_buffers <- function(gps_data,
buffers,
known_nest_coords,
min_pts,
sea_start,
sea_end,
nest_cycle,
min_d_fix,
min_consec,
min_top_att,
min_days_att,
discard_overlapping = TRUE) {
# Check format of input data
check_input(gps_data)
# Record the start time
start_time <- Sys.time()
# Create a folder to store the outputs
# Temporary folder name
timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
temp_name <- paste0("output/temp_", timestamp)
dir.create(temp_name, showWarnings = FALSE, recursive = TRUE)
# Initialize error log
cat("Error log \n\n", file = paste0("output/errorlog", timestamp, ".txt"))
# Create a vector of bursts to loop through
bursts <- unique(gps_data$burst)
for (i in 1:length(bursts)) {
burst_id <- bursts[i]
cat("****************************\n")
cat(paste0("Burst ", i, " of ", length(bursts), "\n"))
dat <- gps_data %>%
filter(burst==burst_id)
# Print current burst
cat("****************************\n")
cat(paste0("Processing ", burst_id, "\n"))
cat("****************************\n\n")
# Order by date and assign location id
dat <- dat %>%
arrange(date) %>%
mutate(loc_id = 1:nrow(.)) %>%
dplyr::select(loc_id, everything())
# Handle dates
dates_out <- date_handler(dat, sea_start, sea_end)
# dates_out is a list; pull the data from it
dat <- dates_out$dat
# Handle cases where there are no data within the nesting season
if (nrow(dat) == 0) {
cat(paste0("Burst ", burst_id,
": no data available within the nesting season\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Calculate distance matrix
cat("Calculating distance matrix... ")
dmat <- dist_mat(dat)
cat("Done.\n")
# Get potential nest candidates
cat("Finding candidate nest locations for each buffer size... ")
cands_list <- get_candidates_multi(dmat = dmat, buffers = buffers, min_pts = min_pts)
cat("Done.\n")
# Remove distance matrix to free up RAM
rm(dmat)
gc()
# Create list to save results for each buffer
res_buffer <- list()
for (k in 1:length(cands_list)) {
# Need to start with a fresh 'dat' at every k
dat_buff <- dat
# Handle cases where there are no candidates
if (nrow(cands_list[[k]]) == 0) {
cat(paste0("Burst ", burst_id,
": no revisited locations found\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Summarize candidates
cands_count <- candidate_summary(cands = cands_list[[k]])
# Join group ID back to the original data
dat_buff <- left_join(dat, cands_list[[k]], by = "loc_id")
# Save computation time: discard group IDs that appear on < 2 days
keepers <- dat_buff %>%
group_by(group_id, reldate) %>%
tally() %>%
filter(n >= 2) %>%
pull(group_id) %>%
unique()
# Subset data for group_ids of interest
sub <- dat_buff %>%
filter(group_id %in% keepers)
# Handle cases where there are no keepers
if (nrow(sub) == 0) {
cat(paste0("Burst ", burst_id,
": no locations revisited for more than ",
min_consec,
" days\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Calculate revisitation stats
cat(paste0("Calculating revisitation patterns for buffer ", k, " of ", length(buffers), "... "))
daily_stats <- revisit_stats(dat = dat_buff,
sub = sub,
sea_start = sea_start,
sea_end = sea_end,
min_d_fix = min_d_fix,
min_consec = min_consec,
nest_cycle = nest_cycle)
cat("Done.\n")
# Filter group_ids that satisfy input criteria and add coordinates
nests <- daily_stats %>%
filter(!is.na(attempt_start),
!is.na(attempt_end),
consec_days >= min_consec,
perc_days_vis >= min_days_att,
perc_top_vis >= min_top_att) %>%
left_join(dplyr::select(dat_buff, loc_id, long, lat), by = c("group_id" = "loc_id")) %>%
mutate(attempt_start = ymd(dates_out$actual_start) + attempt_start) %>%
mutate(attempt_end = ymd(dates_out$actual_start) + attempt_end) %>%
mutate(burst = burst_id) %>%
dplyr::select(burst,
loc_id = group_id,
long,
lat,
first_date,
last_date,
attempt_start,
attempt_end,
tot_vis,
days_vis,
consec_days,
perc_days_vis,
perc_top_vis) %>%
arrange(desc(tot_vis))
# Handle cases where no nests passed the filter
if (nrow(sub) == 0) {
cat(paste0("Burst ", burst_id,
": no locations found for the specified set of parameters\n"),
file = paste0("output/errorlog", timestamp, ".txt"),
append = TRUE)
next()
}
# Optional: deal with temporally overlapping attempts
if (discard_overlapping) {
nests <- choose_overlapping(nests)
}
# Store results for the current buffer
res_buffer[[k]] <- nests
}
names(res_buffer) <- buffers
cat("\nProcess completed!\n\n")
saveRDS(res_buffer, paste0(temp_name, "/buffer_comparison_", burst_id, ".rds"))
rm(res_buffer)
gc()
}
cat("\nCompleted processing of all bursts.\n")
# At the end
# Combine results
# Each file (per burst) is a list with an element per buffer.
# Convert into a list with an element per buffer where each element is a data.frame for all bursts.
files_buffers <- list.files(temp_name, pattern = "buffer_comparison_", full.names = TRUE)
buffercomp <- list(NA, NA)
for (f in files_buffers) {
temp <- readRDS(f)
buffercomp <- purrr::map2(buffercomp, temp, rbind)
}
buffercomp <- lapply(buffercomp, FUN = function(x){x <- x[-1,]; rownames(x) <- 1:nrow(x); return(x)})
names(buffercomp) <- buffers
# Delete temporary folder and files within it
file.remove(files_buffers)
file.remove(temp_name)
# Count results and compute performance metrics
#Total number of nests
tot_nests <- data.frame()
for (n in 1:(length(buffercomp))) {
temp <- cbind.data.frame(buffer_size = names(buffercomp[n]), n = nrow(buffercomp[[n]]))
tot_nests <- rbind(tot_nests, temp)
}
#Number of nests per individual
nests_per_ind <- data.frame()
for (n in 1:(length(buffercomp))) {
temp <- buffercomp[[n]] %>%
group_by(burst) %>%
tally() %>%
as.data.frame() %>%
cbind(buffer_size = names(buffercomp[n])) %>%
dplyr::select(burst, buffer_size, n)
nests_per_ind <- rbind(nests_per_ind, temp) %>%
arrange(burst)
}
#If known nests are provided, also return performance metrics
if (!is.null(known_nest_coords)) {
# Fix column names
names(known_nest_coords) <- c("burst", "true_long", "true_lat")
# Change burst to character if it is not already
known_nest_coords$burst <- as.character(known_nest_coords$burst)
#Get rid of bursts that don't have at least 'min_consec' days of data
enough <- gps_data %>%
group_by(burst) %>%
summarize(days_data = length(unique(date(date)))) %>%
filter(days_data >= min_consec) %>%
pull(burst)
known_nest_coords <- known_nest_coords %>%
filter(burst %in% enough)
buffercomp_copy <- lapply(buffercomp, FUN = function(x){
y <- x[x$burst %in% enough,]
return(y)
})
#Positive predictive value
# Number of known nests found
ppv_num <- unlist(lapply(buffercomp_copy, FUN = function(x) {
y <- x %>%
left_join(known_nest_coords, by = "burst") %>%
mutate(dist = geosphere::distGeo(cbind(true_long, true_lat), cbind(long, lat))) %>%
filter(dist <= 100) %>%
nrow()
return(y)
}))
# Total number of nests found
ppv_den <- unlist(lapply(buffercomp_copy, FUN = function(x) {
y <- x %>%
filter(burst %in% known_nest_coords$burst) %>%
nrow()}))
# PPV
ppv <- data.frame(ppv = ppv_num/ppv_den*100)
ppv <- cbind(ppv, buffer_size = rownames(ppv)) %>%
dplyr::select(buffer_size, ppv)
rownames(ppv) <- NULL
# Sensitivity
# Numbr of known nests found
sen_num <- ppv_num
# Number of nests we expected to find
sen_den <- rep(nrow(known_nest_coords), length(ppv_num))
# Sensitivity
sens <- data.frame(sens = sen_num/sen_den*100)
sens <- cbind(sens, buffer_size = rownames(sens)) %>%
dplyr::select(buffer_size, sens)
rownames(sens) <- NULL
# False negatives
# Number of known nests we failed to find
fn_num <- sen_den - sen_num
# Number of nests we expected to find
fn_den <- sen_den
# False negative rate
fn <- data.frame(fn = fn_num/fn_den*100)
fn <- cbind(fn, buffer_size = rownames(fn)) %>%
dplyr::select(buffer_size, fn)
rownames(fn) <- NULL
}
# Append performance metrics to output
buffercomp$tot_nests <- tot_nests
buffercomp$nests_per_ind <- nests_per_ind
if (!is.null(known_nest_coords)) {
buffercomp$ppv <- ppv
buffercomp$sens <- sens
buffercomp$fn <- fn
}
#Record the end time
end_time <- Sys.time()
#Calculate the difference
total_time <-  difftime(end_time, start_time, units = "min") %>%
as.numeric() %>%
round(1)
#Report time
cat(paste0("\nTotal processing time: ", total_time, " minutes.\n"))
# Return results
return(buffercomp)
}
prova <- compare_buffers(gps_data = woodstorks, buffers = c(5, 2000), min_pts = min_pts, sea_start = sea_start, sea_end = sea_end, min_d_fix = min_d_fix,
nest_cycle = nest_cycle, min_consec = min_consec, min_top_att = min_top_att, min_days_att = min_days_att, discard_overlapping = T, known_nest_coords = known_nest_coords)
prova
library(nestR)
list(NA, NA)
list(NA, length.out = 10)
list(NA)
rep(NA, length.out = 2)
rep(NA, length.out = 3)
as.list(rep(NA, length.out = 3))
library(nestR)
?date()
as.Date()
?as.Date()
library(nestR)
library(nestR)
library(nestR)
